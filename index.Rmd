---
title: "Houston Crime Descriptive and Exploratory Analysis using R"
author: "Saul Cruz"
date: "4/25/2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objective

The objective of this post is to do an exploratory analysis of crime in Houston, Texas. There are many posts out there that visualize similar patterns (See References). I'll use different tools and packages in R like ggplot2 and ggmap. This is my first post of a series about Crime Analysis using R. This post will not only help us to identify the right questions, but to learn what we can do in R following the Epicycle of Analysis. This post will try to define and answer descriptive and exploratory questions. In other words we'll summarize a characteristic of a crime dataset (Socrata) and analyze the data to see if there are patterns, trends, or relationships. In future posts of this series I will define and answer inferential, predictive, causal and mechanistic questions.

##Pre-requisites
Installed the following packages:

devtools::install_github(gm"dkahle/ggmap")

devtools::install_github("hadley/ggplot2@v2.2.0")

## Dataset

The dataset used for this exploratory analysis can be found in [Socrata-Harris County Sheriff's Office](https://moto.data.socrata.com/dataset/Harris-County-Sheriff-s-Office/p6kq-vsa3)

We use the  lubridate package to convert datetime fields

```{r dataset_1, message=FALSE}
library(RCurl);library(lubridate);library(ggmap);library(plyr);library(dplyr);library(ggplot2)
dataset<-getURL('https://moto.data.socrata.com/api/views/p6kq-vsa3/rows.csv?accessType=DOWNLOAD',ssl.verifyhost=FALSE,ssl.verifypeer=FALSE)
data <- read.csv(textConnection(dataset), header=T, na.strings = c('#DIV/0', '', 'NA') ,stringsAsFactors = F)
str(data)
observations<-NROW(data)
```

The original dataset as of today (4/11/2017), contains `r observations` observations and 33 variables

## Data Cleansing
These are the steps followed to clean the data, make it more readable and relevant. This is not feature selection since we're not predicting yet.

1. Convert date attributes from char to datetime, create incident_year, incident_month columns, and char to factor

```{r cleaning_1, message=FALSE,warning=FALSE}
data$incident_datetime<-mdy_hms(data$incident_datetime)
data$created_at<-mdy_hms(data$created_at)
data$updated_at<-mdy_hms(data$updated_at)
data$year_incident<-year(data$incident_datetime)
data$month_incident<-month(data$incident_datetime)
data$day_of_week<-as.factor(data$day_of_week)
data$day_of_week<-factor(data$day_of_week,levels = c("Sunday","Monday", 
                                                           "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")) #Note that we order the days
```

2. Remove elements with a High Missing Value Ratio (>75%)
```{r cleaning_2, message=FALSE,warning=FALSE}
data<-data[,-which(colMeans(is.na(data)) > 0.75)]
```
After this step 11 variables are removed. 

3. Filtering out more incident types

For this analysis we'll include the top 20 primary incident types, however from these 20 we will exclude the following incident types: Simple Assault (Family), Juvenile Runaways, Terroristic Threat, Possession of Marijuana, Fraud, Forgery, Credit or Debit Card abuse

```{r cleansing_3, message=FALSE,warning=FALSE}
 count_by_incident_primary<-data %>% group_by(incident_type_primary) %>% tally(sort = TRUE) ##used for reporting and cleansing
 top20_primary<-count_by_incident_primary[1:20,] 
 data<-data[data$incident_type_primary %in% top20_primary$incident_type_primary,]
 out<-c("[RMS] Simple Assault (Family)","[RMS] Juvenile Runaways","[RMS] Terroristic Threat","[RMS] Credit or Debit Card abuse","[RMS] Possession of Marijuana","[RMS] Fraud","[RMS] Forgery", "[RMS] Recovered Vehicles (Outside Agency Stolen)")
 data<-data[!data$incident_type_primary %in% out,]
```

4.-Remove [RMS] string

```{r cleansing_4, message=FALSE,warning=FALSE}
data$incident_type_primary<-gsub('\\[RMS\\] ','',data$incident_type_primary)

```

Note that while we did some data cleansing to ensure data quality, the data set may still contain errors.

##Defining the question

Assuming that the data source is correct, I'm very interested in the area I live, for example, Midtown Houston, if I go out with my wife or friends, I'd like to prevent any kind of crime against us, for instance, leaving my car parked in some area, or going out for vacation and leaving my apartment without any kind of alarm system. I'm not a police officer so I'll try to make data-driven decisions. To do this, my objective will be to answer the following questions:

* What are the most common crime incidents in Houston,TX and Midtown Houston?

* What/Where are the most affected areas in Midtown, Houston, TX? (We need to define Area, hotspots, blocks?) -Descriptive

* When are these areas being affected?

* How often are these areas/blocks/hotspots affected?

Combining all questions we'd like to answer the likelihood of being affected by crime at a particular area and time

##Exploratory Analysis


### Top 20 Primary Incident Type in Harris County, TX (All Time)

```{r exploratory_2, message=FALSE,warning=FALSE}
        top20_primary
```

###Midtown data
Since we are only interested in crimes which take place midtown, we need to restrict the data set. As stated in Kahle & Wickham's [article](https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf) To determine a bounding box, we use google maps to define the top and bottom boundaries (lats and lons), then we create the map using [qmap from ggmap package](https://cran.r-project.org/web/packages/ggmap/ggmap.pdf)


```{r exploratory_3, message=FALSE,warning=FALSE}

##top boundary    29.757992, -95.361392 (Downtown Area/Discovery Green)
##bottom boundary 29.725616, -95.402247 (Museum District)
midtown<-data[-95.402247<=data$longitude & data$longitude<=-95.361392 & 29.725616<=data$latitude & data$latitude<=29.757992,]

```
Midtown data contains data from 2009-2017

###All time Midtown data HotSpots using stat_density2d

The visualization below points out that there are around 4 hotspots of crime activity in Midtown
```{r exploratory_4, message=FALSE,warning=FALSE}

MidtownMap<-qmap(location="midtown houston",zoom=14,legend="bottom")
MidtownMap+
        stat_density2d(
                aes(x=longitude,y=latitude, fill=..level..,alpha= ..level..),
                size=2,bins=6,data=midtown,
                geom="polygon"
        )+
        scale_fill_gradient(low="black",high="red")
```

These 4 hotspots are located in:

* From Bagby St to San Jacinto St. via Webster St (The Heart of Midtown)
* Around San Jacinto and Eagle St   (Reference: Fiesta Mart)
* Sul Ross St and Yupon St          (Reference: University of St. Thomas Area)
* Cleveland St and Gillete St       (Reference: Carnegie Vanguard High School)

###Midtown crime data points using geom_point()

Even though this visualization is not very intuitive, we can identify the density using the real crime incidents by incident type.

```{r exploratory_5, message=FALSE,warning=FALSE}

MidtownMap<-qmap("midtown houston",zoom=14,maptype="toner",
                    source="stamen",legend="right")

MidtownMap+
        geom_point(
                aes(x=longitude,y=latitude,colour=incident_type_primary),
                size=2,data=midtown
        )+theme(legend.title = element_blank())

```

###All time Midtown hotspots by day of week

We can clearly see that the density is higher on Mondays for the heart of midtown area. However, looks like it is more likely to be affected by crime at the Fiesta Mart area on Thursdays. And the St. Thomas University Area is more dangerous on Tuesday. 

```{r exploratory_6, message=FALSE,warning=FALSE}

MidtownMap<-qmap(location="midtown houston",zoom=14,legend="bottom")
MidtownMap+
        stat_density2d(
                aes(x=longitude,y=latitude, fill=..level..,alpha= ..level..),
                size=2,bins=6,data=midtown,
                geom="polygon"
        )+
        scale_fill_gradient(low="black",high="red")+
        facet_wrap(~ day_of_week)
```

Of course, this is all time data, meaning that the temporal information may change over time, it does not necessarily represent the current density. We should take into account temporal patterns. Also, we need to consider more details, like type of incidents that are more likely to happen at certain times of the day than others.

##References

There are many resources and posts out there that are vital to understand R and Crime analysis. You can find below all the references used in this post.

[How to deal with date and time in R](https://rstudio-pubs-static.s3.amazonaws.com/28038_1bcb9aa80ca84f27ace07d612872861a.html)

[Visualising Crime Hotspots in England and Wales using ggmap](https://www.r-bloggers.com/visualising-crime-hotspots-in-england-and-wales-using-ggmap-2/)

[ggmap: Spatial Visualization with ggplot2](http://stat405.had.co.nz/ggmap.pdf)

[Aggregating and analyzing data with dplyr](http://kbroman.org/datacarpentry_R_2016-06-01/03-dplyr.html)

[Heatmap of 2d bin counts](http://ggplot2.tidyverse.org/reference/geom_bin2d.html)

[Kahle & Wikcham. (2013). "ggmap: Spatial Visualization with ggplot2". The R Journal, Vol 5](https://journal.r-project.org/archive/2013-1/kahle-wickham.pdf)

[Townsley](https://research-repository.griffith.edu.au/bitstream/handle/10072/25749/57166_1.pdf%3bsequence=1)




